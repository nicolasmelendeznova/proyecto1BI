{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KuCeFYhKL1t"
      },
      "source": [
        "###Descarga e importar librerias\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoxQrOQSJopP",
        "outputId": "f3bac0d4-fd14-4bad-c6c2-28c268edd702"
      },
      "source": [
        "  !pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 43.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85448 sha256=7178038a402cb7e9659d985cb6bece85157952545b6cb3877938d51e34c2ec15\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dZeb09fkqcl"
      },
      "source": [
        "# Importa la libreria \n",
        "import nltk\n",
        "# Descarga paquetes adicionales"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU7ybRRF0ctA"
      },
      "source": [
        "# Importación de librerias\n",
        "seed = 161\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "import re, string, unicodedata\n",
        "import contractions\n",
        "import inflect\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, plot_precision_recall_curve\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "# Para crear el arbol de decisión \n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Para evaluar el modelo\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score \n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "# Regresion lineal\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# Preprocesar\n",
        "from sklearn.preprocessing import StandardScaler, Normalizer\n",
        "# Balanceo\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Para evaluar el modelo\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "# Seaborn\n",
        "import seaborn as sns \n",
        "from sklearn import tree\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkbbD-TSklZk",
        "outputId": "49b32f55-7465-41d7-dd35-f07098c732c8"
      },
      "source": [
        "# Punkt permite separar un texto en frases.\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY9V5QrHkt95",
        "outputId": "14bae6f4-a5ac-4de5-bf0c-f517bf243e91"
      },
      "source": [
        "# Descarga todas las palabras vacias, es decir, aquellas que no aportan nada al significado del texto\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnEsi1Btkv_6",
        "outputId": "b38ad799-bc4b-4b9a-cf38-86dc7db967e4"
      },
      "source": [
        "# Descarga de paquete WordNetLemmatizer, este es usado para encontrar el lema de cada palabra\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrMqgCBbKRbE"
      },
      "source": [
        "###Lectura y perfilamiento de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFdfp7o4fUpS",
        "outputId": "1013175f-6322-439f-a451-80da87492f16"
      },
      "source": [
        "data_train=pd.read_csv('/content/train.txt', sep=';', encoding = 'utf-8',header=None)\n",
        "# Dimensiones de los datos\n",
        "data_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfEr0yq3hvUj",
        "outputId": "4db8dc05-7dc5-4c42-933e-d10e6f48091f"
      },
      "source": [
        "data_test=pd.read_csv('/content/test.txt', sep=';', encoding = 'utf-8',header=None)\n",
        "# Dimensiones de los datos\n",
        "data_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t-_QSQ-hv0G",
        "outputId": "45e78850-c5ad-490b-b36d-57da7bcfcf61"
      },
      "source": [
        "data_val=pd.read_csv('/content/val.txt', sep=';', encoding = 'utf-8',header=None)\n",
        "# Dimensiones de los datos\n",
        "data_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC28pGvlpfBB"
      },
      "source": [
        "data_train[1]=data_train[1].replace({'sadness': 1, 'anger': 2, 'love': 3, 'joy': 4, 'fear': 5, 'surprise':6})\n",
        "data_test[1]=data_test[1].replace({'sadness': 1, 'anger': 2, 'love': 3, 'joy': 4, 'fear': 5, 'surprise':6})\n",
        "data_val[1]=data_val[1].replace({'sadness': 1, 'anger': 2, 'love': 3, 'joy': 4, 'fear': 5, 'surprise':6})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk8jLLHhKngY"
      },
      "source": [
        "textos = data_train.copy()\n",
        "textos['Conteo'] = [len(x) for x in textos[0]]\n",
        "textos['Moda'] = [max(set(x.split(' ')), key = x.split(' ').count) for x in textos[0]]\n",
        "textos['Max'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos[0]]\n",
        "textos['Min'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKHZsAz1qSlc"
      },
      "source": [
        "textos = data_test.copy()\n",
        "textos['Conteo'] = [len(x) for x in textos[0]]\n",
        "textos['Moda'] = [max(set(x.split(' ')), key = x.split(' ').count) for x in textos[0]]\n",
        "textos['Max'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos[0]]\n",
        "textos['Min'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8r8k0-eqS76"
      },
      "source": [
        "textos = data_val.copy()\n",
        "textos['Conteo'] = [len(x) for x in textos[0]]\n",
        "textos['Moda'] = [max(set(x.split(' ')), key = x.split(' ').count) for x in textos[0]]\n",
        "textos['Max'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos[0]]\n",
        "textos['Min'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujmcX0iGhJof"
      },
      "source": [
        "# Se realiza un perfilamiento de los datos con la libre pandas profiling\n",
        "#ProfileReport(textos)\n",
        "\n",
        "# Nota: Este comando puede generar el siguiente error: \n",
        "# ImportError: cannot import name 'ABCIndexClass' from 'pandas.core.dtypes.generic'\n",
        "# Para solucionarlo, basta instalar cualquie version 1.2.x de Pandas: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHmH2JIPh7l-"
      },
      "source": [
        "###Preparacion de datos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OPRY4rIjb0g"
      },
      "source": [
        "limpieza\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeJ8tcQAh7ZG"
      },
      "source": [
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "    \n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def preprocessing(words):\n",
        "    words = to_lowercase(words)\n",
        "    words = replace_numbers(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeQdnCUFjdlm"
      },
      "source": [
        "tokenizacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTby_dFZiIM9"
      },
      "source": [
        "#Aplica la corrección de las contracciones\n",
        "data_train[0] = data_train[0].apply(contractions.fix) \n",
        "data_test[0] = data_test[0].apply(contractions.fix) \n",
        "data_val[0] = data_val[0].apply(contractions.fix) \n",
        " #Aplica la eliminación del ruido\n",
        "data_train['words'] = data_train[0].apply(word_tokenize).apply(preprocessing)\n",
        "data_test['words'] = data_test[0].apply(word_tokenize).apply(preprocessing)\n",
        "data_val['words'] = data_val[0].apply(word_tokenize).apply(preprocessing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK3P5rLelQk8"
      },
      "source": [
        "normalizacion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyHIQRsglSX8"
      },
      "source": [
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "#    stemmer = LancasterStemmer()\n",
        "#    stems = []\n",
        "#    for word in words:\n",
        "#        stem = stemmer.stem(word)\n",
        "#        stems.append(stem)\n",
        "#    return stems\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def stem_and_lemmatize(words):\n",
        "    lemmas = lemmatize_verbs(words)\n",
        "    return lemmas\n",
        "\n",
        "#Aplica lematización y Eliminación de Prefijos y Sufijos.\n",
        "data_train['words'] = data_train['words'].apply(stem_and_lemmatize) \n",
        "data_test['words'] = data_test['words'].apply(stem_and_lemmatize) \n",
        "data_val['words'] = data_val['words'].apply(stem_and_lemmatize) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm-jmeqrA6jb",
        "outputId": "545c9939-af90-455a-8b3e-81fdfab5a5d2"
      },
      "source": [
        "data_train['words']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                        [feel, humiliate]\n",
              "1        [go, feel, hopeless, damn, hopeful, around, so...\n",
              "2                [grab, minute, post, feel, greedy, wrong]\n",
              "3        [ever, feel, nostalgic, fireplace, know, still...\n",
              "4                                          [feel, grouchy]\n",
              "                               ...                        \n",
              "15995    [brief, time, beanbag, say, anna, feel, like, ...\n",
              "15996    [turn, feel, pathetic, still, wait, table, sub...\n",
              "15997                        [feel, strong, good, overall]\n",
              "15998                    [feel, like, rude, comment, glad]\n",
              "15999                   [know, lot, feel, stupid, portray]\n",
              "Name: words, Length: 16000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50bV1yngoJ5a"
      },
      "source": [
        "seleccion de campos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WnipkbCn757"
      },
      "source": [
        "data_train['processed_message'] = data_train['words'].apply(lambda x: ' '.join(map(str, x)))\n",
        "X_data= data_train['processed_message']\n",
        "y_data = data_train[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Ld7RM2pFnV"
      },
      "source": [
        "matriz de conteo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JJPfM01oxU9",
        "outputId": "9642bda2-f0e8-4b32-dae8-fe3ab8bb63a4"
      },
      "source": [
        "count = CountVectorizer()\n",
        "X_count = count.fit_transform(X_data)\n",
        "print(X_count.shape)\n",
        "X_count.toarray()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16000, 12061)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXX8n15_k43c",
        "outputId": "c666a4a4-5a01-492b-f250-7a209601e54f"
      },
      "source": [
        "data_test['processed_message'] = data_test['words'].apply(lambda x: ' '.join(map(str, x)))\n",
        "X_test = count.fit_transform(data_test['processed_message'])\n",
        "print(X_test.shape)\n",
        "X_test.toarray()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 3808)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDXTnVFufcgn"
      },
      "source": [
        "Matriz TF-ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tAtXj-kfgU0"
      },
      "source": [
        " vectorizer = TfidfVectorizer()\n",
        " X_ID = vectorizer.fit_transform(X_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jvGX-zCgQYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4abf5c1-cbea-4e7e-9784-54af190fc6a0"
      },
      "source": [
        "print(X_ID)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 5090)\t0.9889069406128232\n",
            "  (0, 3887)\t0.14853640229851484\n",
            "  (1, 767)\t0.44540584507336783\n",
            "  (1, 1524)\t0.32075393333556007\n",
            "  (1, 9854)\t0.3043171823000295\n",
            "  (1, 580)\t0.29147433552867913\n",
            "  (1, 5014)\t0.3888011381197094\n",
            "  (1, 2517)\t0.4127373380836858\n",
            "  (1, 5016)\t0.3840759022899946\n",
            "  (1, 4477)\t0.22034234972378397\n",
            "  (1, 3887)\t0.05984000882412842\n",
            "  (2, 11937)\t0.3905048348651996\n",
            "  (2, 4574)\t0.43381526581073987\n",
            "  (2, 8178)\t0.362690236072352\n",
            "  (2, 6772)\t0.4978583161827672\n",
            "  (2, 4522)\t0.5247650992815985\n",
            "  (2, 3887)\t0.0672664057154502\n",
            "  (3, 8387)\t0.5458231231359385\n",
            "  (3, 10123)\t0.25906570528480016\n",
            "  (3, 5922)\t0.22822561411666759\n",
            "  (3, 3987)\t0.5592502350171039\n",
            "  (3, 7302)\t0.40491074463378485\n",
            "  (3, 3609)\t0.320064563124414\n",
            "  (3, 3887)\t0.06099415219424682\n",
            "  (4, 4603)\t0.9907100239386764\n",
            "  :\t:\n",
            "  (15995, 10756)\t0.20596454482266746\n",
            "  (15995, 3887)\t0.054234349363171765\n",
            "  (15996, 10233)\t0.47460177710630275\n",
            "  (15996, 2666)\t0.40197172855899893\n",
            "  (15996, 10456)\t0.38317499332382915\n",
            "  (15996, 10531)\t0.3573543128835534\n",
            "  (15996, 11602)\t0.3061349653436074\n",
            "  (15996, 11039)\t0.2971042835154101\n",
            "  (15996, 7777)\t0.32963563706842725\n",
            "  (15996, 10123)\t0.2132525236163882\n",
            "  (15996, 3887)\t0.05020794576791229\n",
            "  (15997, 7604)\t0.7006628533705841\n",
            "  (15997, 10196)\t0.5595612879329901\n",
            "  (15997, 4496)\t0.43292144276873806\n",
            "  (15997, 3887)\t0.09242161730027718\n",
            "  (15998, 1993)\t0.5670040379974042\n",
            "  (15998, 9093)\t0.5728576790594957\n",
            "  (15998, 4435)\t0.5344000647845126\n",
            "  (15998, 6171)\t0.23896268229572806\n",
            "  (15998, 3887)\t0.08748661420780746\n",
            "  (15999, 8155)\t0.6991960082353295\n",
            "  (15999, 10223)\t0.4839371999443921\n",
            "  (15999, 6298)\t0.4184151157265773\n",
            "  (15999, 5922)\t0.30833148827802725\n",
            "  (15999, 3887)\t0.08240274780329825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Cvqt5iM49r"
      },
      "source": [
        "Asiganacion de ejes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNadAcstM7P6"
      },
      "source": [
        "# X la matriz y Y el emoji\n",
        "X = X_ID\n",
        "Y = data_train[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGxnQw-RNBDg"
      },
      "source": [
        "# Parte en Entreno y Prueba\n",
        "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "X_train=X\n",
        "X_test= X_test\n",
        "Y_train = Y\n",
        "Y_test = data_test[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA1zIfNEpK7S"
      },
      "source": [
        "###KNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7eVTPMVM_Pa"
      },
      "source": [
        "# Modelo\n",
        "knn = KNeighborsClassifier(n_neighbors=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6VVXA1Plb1z"
      },
      "source": [
        "# Entrenar el modelo\n",
        "knn.fit(X_train,Y_train)\n",
        "\n",
        "# Predicciones\n",
        "y_pred_train = knn.predict(X_train)\n",
        "y_pred_test = knn.predict(X_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdjHkkJD7pWF",
        "outputId": "d94cfa58-9d54-4846-9672-7849e49a7ff1"
      },
      "source": [
        "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "# Lista de Hiperparámetros a afinar\n",
        "n_neighbors = list(range(1,10))\n",
        "p=[1,2] #Función de distancia 1: manhattan, 2: euclidean, otro valor: minkowski\n",
        "\n",
        "#Convert to dictionary\n",
        "hyperparameters = dict(n_neighbors=n_neighbors, p=p)\n",
        "\n",
        "#Create new KNN object\n",
        "knn_2 = KNeighborsClassifier()\n",
        "\n",
        "#Use GridSearch\n",
        "mejor_modelo_knn = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
        "\n",
        "#Fit the model\n",
        "mejor_modelo_knn.fit(X_train, Y_train)\n",
        "\n",
        "#Print The value of best Hyperparameters\n",
        "print('Best p:', mejor_modelo_knn.best_estimator_.get_params()['p'])\n",
        "print('Best n_neighbors:', mejor_modelo_knn.best_estimator_.get_params()['n_neighbors'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best p: 2\n",
            "Best n_neighbors: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQGXwGEDhbBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e3e0e5-676f-4957-9c0e-b77b4186af88"
      },
      "source": [
        "print(classification_report(Y_train, y_pred_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.82      0.94      0.88      4666\n",
            "           2       0.86      0.83      0.85      2159\n",
            "           3       0.84      0.66      0.74      1304\n",
            "           4       0.87      0.90      0.89      5362\n",
            "           5       0.91      0.75      0.82      1937\n",
            "           6       0.82      0.61      0.70       572\n",
            "\n",
            "    accuracy                           0.85     16000\n",
            "   macro avg       0.85      0.78      0.81     16000\n",
            "weighted avg       0.86      0.85      0.85     16000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKXtLTy6pSJL"
      },
      "source": [
        "###Arboles\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_xXhoq2B4xr"
      },
      "source": [
        "# Primero declaramos el preprocesamiento\n",
        "pre_process = [('normalizer', Normalizer()),\n",
        "               ('standarizer', StandardScaler())] "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUWVstswCGm-"
      },
      "source": [
        "# Classificador\n",
        "classfier = [('tree',DecisionTreeClassifier(random_state=seed))]"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0sN_I3Mpber"
      },
      "source": [
        "# Modelo\n",
        "arbol = DecisionTreeClassifier(criterion='entropy', max_depth = 4, random_state=0)\n",
        "\n",
        "# Entrenar el modelo\n",
        "arbol = arbol.fit(X_train,Y_train)\n",
        "\n",
        "# Predicciones\n",
        "y_pred_train = arbol.predict(X_train)\n",
        "y_pred_test = arbol.predict(X_test)\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYs6bfMvNjUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0af9d7e7-d5f1-467f-b7be-ab2a707954db"
      },
      "source": [
        "print(classification_report(Y_train, y_pred_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.50      0.00      0.00      4666\n",
            "           2       1.00      0.00      0.00      2159\n",
            "           3       0.75      0.00      0.00      1304\n",
            "           4       0.34      0.99      0.51      5362\n",
            "           5       0.51      0.07      0.12      1937\n",
            "           6       0.68      0.29      0.40       572\n",
            "\n",
            "    accuracy                           0.35     16000\n",
            "   macro avg       0.63      0.22      0.17     16000\n",
            "weighted avg       0.54      0.35      0.20     16000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lDs1DxSCkO2",
        "outputId": "606ac748-2acd-4f12-ef1c-f2c9bc50d3fb"
      },
      "source": [
        "test = []\n",
        "for i in range(10):\n",
        "    # Split\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=i)\n",
        "    \n",
        "    # Declare\n",
        "    arbol.fit(X_train, Y_train)\n",
        "    y_pred_test = arbol.predict(X_test)\n",
        "    \n",
        "print('F1 Score sobre test: %.3f' % np.mean(test))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score sobre test: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHuEGChKDpBC"
      },
      "source": [
        "plot_confusion_matrix(arbol, X_test, Y_test)  \n",
        "plt.show()"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTPD73wix2R7"
      },
      "source": [
        "##SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDrdJCwZZUi6"
      },
      "source": [
        ">>> from sklearn.svm import LinearSVC\n",
        ">>> from sklearn.pipeline import make_pipeline\n",
        ">>> from sklearn.preprocessing import StandardScaler\n",
        ">>> from sklearn.datasets import make_classification\n",
        "X_train, Y_train = make_classification(n_features=4, random_state=0)\n",
        "clf = make_pipeline(StandardScaler(),\n",
        "                     LinearSVC(random_state=0, tol=1e-5))\n",
        "clf.fit(X_train, Y_train)\n",
        "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
        "                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNqeQDRPZWIg"
      },
      "source": [
        "y_pred_train = clf.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gWf9JAzZXm4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f24004-7a18-4b5e-a748-1def065caa2f"
      },
      "source": [
        "# Metricas\n",
        "print(classification_report(Y_train, y_pred_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93        50\n",
            "           1       0.96      0.90      0.93        50\n",
            "\n",
            "    accuracy                           0.93       100\n",
            "   macro avg       0.93      0.93      0.93       100\n",
            "weighted avg       0.93      0.93      0.93       100\n",
            "\n"
          ]
        }
      ]
    }
  ]
}